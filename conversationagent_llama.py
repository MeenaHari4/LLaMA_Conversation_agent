# -*- coding: utf-8 -*-
"""ConversationAgent_LLaMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SH8q4-95ZCKS_XnUxn5dfrbhilg4ILEE

Installing necessary libararies
"""

!pip install transformers torch accelerate

"""Login to Hugging face"""

!hf auth login

"""Load Model and its tokenizer"""

from transformers import AutoTokenizer
import transformers
import torch

model = "meta-llama/Llama-2-7b-chat-hf" # meta-llama/Llama-2-7b-hf

tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)

"""Creating a userdefined function to return the response generated for user input."""

from transformers import pipeline

# Load the LLaMA model pipeline
llama_pipeline = pipeline(
    "text-generation",
    model="meta-llama/Llama-2-7b-chat-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

def get_llama_response(prompt: str):
    sequences = llama_pipeline(
        prompt,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        truncation = True,
        max_new_tokens=256,
    )
    response = sequences[0]['generated_text'].replace(prompt, "").strip()
    return response

"""Looping the conversation until user uses few keyword(bye,quit,exit) to stop the conversation."""

while True:
    user_input = input("You: ")
    if user_input.lower() in ["bye", "quit", "exit"]:
        print("Chatbot: Goodbye!")
        break
    response = get_llama_response(user_input)
    print("Converational Agent:", response)